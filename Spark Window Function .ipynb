{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@7574773b\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@7574773b"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder.getOrCreate()\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+------+---------------+\n",
      "|  depName|empNo|  name|salary|          hobby|\n",
      "+---------+-----+------+------+---------------+\n",
      "|    sales|    1| Alice|  5000|    [game, ski]|\n",
      "|personnel|    2|Olivia|  3900|    [game, ski]|\n",
      "|    sales|    3|  Ella|  4800|   [skate, ski]|\n",
      "|    sales|    4|  Ebba|  4800|    [game, ski]|\n",
      "|personnel|    5| Lilly|  3500|   [climb, ski]|\n",
      "|  develop|    7|Astrid|  4200|    [game, ski]|\n",
      "|  develop|    8|  Saga|  6000|   [kajak, ski]|\n",
      "|  develop|    9| Freja|  4500|  [game, kajak]|\n",
      "|  develop|   10| Wilma|  5200|    [game, ski]|\n",
      "|  develop|   11|  Maja|  5200|[game, farming]|\n",
      "+---------+-----+------+------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined class Salary\n",
       "empsalary = [depName: string, empNo: bigint ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[depName: string, empNo: bigint ... 3 more fields]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Salary(depName: String, empNo: Long, name: String, \n",
    "    salary: Long, hobby: Seq[String])\n",
    "import spark.implicits._\n",
    "val empsalary = Seq(\n",
    "  Salary(\"sales\",     1,  \"Alice\",  5000, List(\"game\",  \"ski\")),\n",
    "  Salary(\"personnel\", 2,  \"Olivia\", 3900, List(\"game\",  \"ski\")),\n",
    "  Salary(\"sales\",     3,  \"Ella\",   4800, List(\"skate\", \"ski\")),\n",
    "  Salary(\"sales\",     4,  \"Ebba\",   4800, List(\"game\",  \"ski\")),\n",
    "  Salary(\"personnel\", 5,  \"Lilly\",  3500, List(\"climb\", \"ski\")),\n",
    "  Salary(\"develop\",   7,  \"Astrid\", 4200, List(\"game\",  \"ski\")),\n",
    "  Salary(\"develop\",   8,  \"Saga\",   6000, List(\"kajak\", \"ski\")),\n",
    "  Salary(\"develop\",   9,  \"Freja\",  4500, List(\"game\",  \"kajak\")),\n",
    "  Salary(\"develop\",   10, \"Wilma\",  5200, List(\"game\",  \"ski\")),\n",
    "  Salary(\"develop\",   11, \"Maja\",   5200, List(\"game\",  \"farming\"))).toDS\n",
    "empsalary.createTempView(\"empsalary\")\n",
    "empsalary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Finding Avg and total salary over dept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+------+------------------------------+--------------+------------+\n",
      "|depName  |empNo|name  |salary|salaries                      |average_salary|total_salary|\n",
      "+---------+-----+------+------+------------------------------+--------------+------------+\n",
      "|develop  |7    |Astrid|4200  |[4200, 6000, 4500, 5200, 5200]|5020          |25100       |\n",
      "|develop  |8    |Saga  |6000  |[4200, 6000, 4500, 5200, 5200]|5020          |25100       |\n",
      "|develop  |9    |Freja |4500  |[4200, 6000, 4500, 5200, 5200]|5020          |25100       |\n",
      "|develop  |10   |Wilma |5200  |[4200, 6000, 4500, 5200, 5200]|5020          |25100       |\n",
      "|develop  |11   |Maja  |5200  |[4200, 6000, 4500, 5200, 5200]|5020          |25100       |\n",
      "|sales    |1    |Alice |5000  |[5000, 4800, 4800]            |4866          |14600       |\n",
      "|sales    |3    |Ella  |4800  |[5000, 4800, 4800]            |4866          |14600       |\n",
      "|sales    |4    |Ebba  |4800  |[5000, 4800, 4800]            |4866          |14600       |\n",
      "|personnel|2    |Olivia|3900  |[3900, 3500]                  |3700          |7400        |\n",
      "|personnel|5    |Lilly |3500  |[3900, 3500]                  |3700          |7400        |\n",
      "+---------+-----+------+------+------------------------------+--------------+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "overCategory = org.apache.spark.sql.expressions.WindowSpec@220d34ea\n",
       "df = [depName: string, empNo: bigint ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[depName: string, empNo: bigint ... 5 more fields]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val overCategory = Window.partitionBy('depName)\n",
    "val df = empsalary\n",
    "                .withColumn(\"salaries\", collect_list('salary) over overCategory)\n",
    "                .withColumn(\"average_salary\", (avg('salary) over overCategory).cast(\"int\"))\n",
    "                .withColumn(\"total_salary\", sum('salary) over overCategory)\n",
    "        .select(\"depName\", \"empNo\", \"name\", \"salary\", \"salaries\", \"average_salary\", \"total_salary\")\n",
    "df.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+------+------------------------------+--------------+------------+\n",
      "|depName  |empNo|name  |salary|salaries                      |average_salary|total_salary|\n",
      "+---------+-----+------+------+------------------------------+--------------+------------+\n",
      "|develop  |8    |Saga  |6000  |[6000]                        |6000          |6000        |\n",
      "|develop  |10   |Wilma |5200  |[6000, 5200, 5200]            |5466          |16400       |\n",
      "|develop  |11   |Maja  |5200  |[6000, 5200, 5200]            |5466          |16400       |\n",
      "|develop  |9    |Freja |4500  |[6000, 5200, 5200, 4500]      |5225          |20900       |\n",
      "|develop  |7    |Astrid|4200  |[6000, 5200, 5200, 4500, 4200]|5020          |25100       |\n",
      "|sales    |1    |Alice |5000  |[5000]                        |5000          |5000        |\n",
      "|sales    |3    |Ella  |4800  |[5000, 4800, 4800]            |4866          |14600       |\n",
      "|sales    |4    |Ebba  |4800  |[5000, 4800, 4800]            |4866          |14600       |\n",
      "|personnel|2    |Olivia|3900  |[3900]                        |3900          |3900        |\n",
      "|personnel|5    |Lilly |3500  |[3900, 3500]                  |3700          |7400        |\n",
      "+---------+-----+------+------+------------------------------+--------------+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "overCategory = org.apache.spark.sql.expressions.WindowSpec@5bbade82\n",
       "df = [depName: string, empNo: bigint ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[depName: string, empNo: bigint ... 5 more fields]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val overCategory = Window.partitionBy('depName).orderBy('salary desc)\n",
    "val df = empsalary\n",
    "            .withColumn(\"salaries\", collect_list('salary) over overCategory)\n",
    "            .withColumn(\"average_salary\", (avg('salary) over overCategory).cast(\"int\"))\n",
    "            .withColumn(\"total_salary\", sum('salary) over overCategory)\n",
    "    .select(\"depName\", \"empNo\", \"name\", \"salary\", \"salaries\", \"average_salary\", \"total_salary\")\n",
    "df.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://knockdata.github.io/images/spark-window-function-rank-functions.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+------+----+----------+----------+-----+------------+\n",
      "|depName  |empNo|name  |salary|rank|dense_rank|row_number|ntile|percent_rank|\n",
      "+---------+-----+------+------+----+----------+----------+-----+------------+\n",
      "|develop  |8    |Saga  |6000  |1   |1         |1         |1    |0.0         |\n",
      "|develop  |10   |Wilma |5200  |2   |2         |2         |1    |0.25        |\n",
      "|develop  |11   |Maja  |5200  |2   |2         |3         |2    |0.25        |\n",
      "|develop  |9    |Freja |4500  |4   |3         |4         |2    |0.75        |\n",
      "|develop  |7    |Astrid|4200  |5   |4         |5         |3    |1.0         |\n",
      "|sales    |1    |Alice |5000  |1   |1         |1         |1    |0.0         |\n",
      "|sales    |3    |Ella  |4800  |2   |2         |2         |2    |0.5         |\n",
      "|sales    |4    |Ebba  |4800  |2   |2         |3         |3    |0.5         |\n",
      "|personnel|2    |Olivia|3900  |1   |1         |1         |1    |0.0         |\n",
      "|personnel|5    |Lilly |3500  |2   |2         |2         |2    |1.0         |\n",
      "+---------+-----+------+------+----+----------+----------+-----+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "overCategory = org.apache.spark.sql.expressions.WindowSpec@2046c3a6\n",
       "df = [depName: string, empNo: bigint ... 7 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[depName: string, empNo: bigint ... 7 more fields]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val overCategory = Window.partitionBy('depName).orderBy('salary desc)\n",
    "val df = empsalary\n",
    "            .withColumn(\"salaries\", collect_list('salary) over overCategory)\n",
    "            .withColumn(\"rank\", rank() over overCategory)\n",
    "            .withColumn(\"dense_rank\", dense_rank() over overCategory)\n",
    "            .withColumn(\"row_number\", row_number() over overCategory)\n",
    "            .withColumn(\"ntile\", ntile(3) over overCategory)\n",
    "            .withColumn(\"percent_rank\", percent_rank() over overCategory)\n",
    "        .select(\"depName\", \"empNo\", \"name\", \"salary\", \"rank\", \"dense_rank\", \"row_number\", \"ntile\", \"percent_rank\")\n",
    "df.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lag & lead in a group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lag and lead can be used, when we want to get a relative result between rows. The real values we get are depending on the order.\n",
    "\n",
    "lag means getting the value from the previous row; lead means getting the value from the next row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+------+----+----+\n",
      "|depName  |empNo|name  |salary|lead|lag |\n",
      "+---------+-----+------+------+----+----+\n",
      "|develop  |8    |Saga  |6000  |5200|null|\n",
      "|develop  |10   |Wilma |5200  |5200|6000|\n",
      "|develop  |11   |Maja  |5200  |4500|5200|\n",
      "|develop  |9    |Freja |4500  |4200|5200|\n",
      "|develop  |7    |Astrid|4200  |null|4500|\n",
      "|sales    |1    |Alice |5000  |4800|null|\n",
      "|sales    |3    |Ella  |4800  |4800|5000|\n",
      "|sales    |4    |Ebba  |4800  |null|4800|\n",
      "|personnel|2    |Olivia|3900  |3500|null|\n",
      "|personnel|5    |Lilly |3500  |null|3900|\n",
      "+---------+-----+------+------+----+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "overCategory = org.apache.spark.sql.expressions.WindowSpec@492b7040\n",
       "df = [depName: string, empNo: bigint ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[depName: string, empNo: bigint ... 4 more fields]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val overCategory = Window.partitionBy('depname).orderBy('salary.desc)\n",
    "val df = empsalary.withColumn(\n",
    "  \"lead\", lead('salary, 1).over(overCategory)).withColumn(\n",
    "  \"lag\", lag('salary, 1).over(overCategory)).select(\n",
    "      \"depName\", \"empNo\", \"name\", \"salary\", \"lead\", \"lag\")\n",
    "df.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+------+----+----+----------------+-------------------+\n",
      "|  depName|empNo|  name|salary|lead| lag|higher_than_next|lower_than_previous|\n",
      "+---------+-----+------+------+----+----+----------------+-------------------+\n",
      "|  develop|    8|  Saga|  6000|5200|null|             800|               null|\n",
      "|  develop|   10| Wilma|  5200|5200|6000|               0|                800|\n",
      "|  develop|   11|  Maja|  5200|4500|5200|             700|                  0|\n",
      "|  develop|    9| Freja|  4500|4200|5200|             300|                700|\n",
      "|  develop|    7|Astrid|  4200|null|4500|            null|                300|\n",
      "|    sales|    1| Alice|  5000|4800|null|             200|               null|\n",
      "|    sales|    3|  Ella|  4800|4800|5000|               0|                200|\n",
      "|    sales|    4|  Ebba|  4800|null|4800|            null|                  0|\n",
      "|personnel|    2|Olivia|  3900|3500|null|             400|               null|\n",
      "|personnel|    5| Lilly|  3500|null|3900|            null|                400|\n",
      "+---------+-----+------+------+----+----+----------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "diff = [depName: string, empNo: bigint ... 6 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[depName: string, empNo: bigint ... 6 more fields]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val diff = df.withColumn(\n",
    "    \"higher_than_next\", 'salary - 'lead).withColumn(\n",
    "    \"lower_than_previous\", 'lag - 'salary)\n",
    "diff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+------+----+----+----------------+-------------------+\n",
      "|  depName|empNo|  name|salary|lead| lag|higher_than_next|lower_than_previous|\n",
      "+---------+-----+------+------+----+----+----------------+-------------------+\n",
      "|  develop|    8|  Saga|  6000|5200|null|             800|                  0|\n",
      "|  develop|   10| Wilma|  5200|5200|6000|               0|                800|\n",
      "|  develop|   11|  Maja|  5200|4500|5200|             700|                  0|\n",
      "|  develop|    9| Freja|  4500|4200|5200|             300|                700|\n",
      "|  develop|    7|Astrid|  4200|null|4500|               0|                300|\n",
      "|    sales|    1| Alice|  5000|4800|null|             200|                  0|\n",
      "|    sales|    3|  Ella|  4800|4800|5000|               0|                200|\n",
      "|    sales|    4|  Ebba|  4800|null|4800|               0|                  0|\n",
      "|personnel|    2|Olivia|  3900|3500|null|             400|                  0|\n",
      "|personnel|    5| Lilly|  3500|null|3900|               0|                400|\n",
      "+---------+-----+------+------+----+----+----------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "diff = [depName: string, empNo: bigint ... 6 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[depName: string, empNo: bigint ... 6 more fields]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val diff = df.withColumn(\n",
    "  \"higher_than_next\", when('lead.isNull, 0).otherwise('salary - 'lead)).withColumn(\n",
    "  \"lower_than_previous\", when('lag.isNull, 0).otherwise('lag - 'salary))\n",
    "diff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----+------+----+---+----------------+-------------------+\n",
      "|depName|empNo|name|salary|lead|lag|higher_than_next|lower_than_previous|\n",
      "+-------+-----+----+------+----+---+----------------+-------------------+\n",
      "+-------+-----+----+------+----+---+----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diff.filter('higher_than_next > (lit(2) * 'salary)).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Total means adding everything up until the currentRow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+------+----+-----+\n",
      "|depName  |empNo|name  |salary|rank|costs|\n",
      "+---------+-----+------+------+----+-----+\n",
      "|develop  |8    |Saga  |6000  |1   |6000 |\n",
      "|develop  |10   |Wilma |5200  |2   |16400|\n",
      "|develop  |11   |Maja  |5200  |2   |16400|\n",
      "|develop  |9    |Freja |4500  |4   |20900|\n",
      "|develop  |7    |Astrid|4200  |5   |25100|\n",
      "|sales    |1    |Alice |5000  |1   |5000 |\n",
      "|sales    |3    |Ella  |4800  |2   |14600|\n",
      "|sales    |4    |Ebba  |4800  |2   |14600|\n",
      "|personnel|2    |Olivia|3900  |1   |3900 |\n",
      "|personnel|5    |Lilly |3500  |2   |7400 |\n",
      "+---------+-----+------+------+----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "overCategory = org.apache.spark.sql.expressions.WindowSpec@4779c0a6\n",
       "running_total = [depName: string, empNo: bigint ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[depName: string, empNo: bigint ... 4 more fields]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val overCategory = Window.partitionBy('depname).orderBy('salary desc)\n",
    "\n",
    "val running_total = empsalary.withColumn(\n",
    "  \"rank\", rank().over(overCategory)).withColumn(\n",
    "  \"costs\", sum('salary).over(overCategory)).select(\n",
    "  \"depName\", \"empNo\", \"name\", \"salary\", \"rank\", \"costs\")\n",
    "running_total.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+------+----------+-----+\n",
      "|depName  |empNo|name  |salary|row_number|costs|\n",
      "+---------+-----+------+------+----------+-----+\n",
      "|develop  |8    |Saga  |6000  |1         |6000 |\n",
      "|develop  |10   |Wilma |5200  |2         |11200|\n",
      "|develop  |11   |Maja  |5200  |3         |16400|\n",
      "|develop  |9    |Freja |4500  |4         |20900|\n",
      "|develop  |7    |Astrid|4200  |5         |25100|\n",
      "|sales    |1    |Alice |5000  |1         |5000 |\n",
      "|sales    |3    |Ella  |4800  |2         |9800 |\n",
      "|sales    |4    |Ebba  |4800  |3         |14600|\n",
      "|personnel|2    |Olivia|3900  |1         |3900 |\n",
      "|personnel|5    |Lilly |3500  |2         |7400 |\n",
      "+---------+-----+------+------+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "overCategory = org.apache.spark.sql.expressions.WindowSpec@741bfbca\n",
       "overRowCategory = org.apache.spark.sql.expressions.WindowSpec@2c38290d\n",
       "running_total = [depName: string, empNo: bigint ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[depName: string, empNo: bigint ... 4 more fields]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val overCategory = Window.partitionBy('depname).orderBy('salary desc)\n",
    "val overRowCategory = Window.partitionBy('depname).orderBy('row_number)\n",
    "\n",
    "val running_total = empsalary.withColumn(\n",
    "  \"row_number\", row_number() over overCategory).withColumn(\n",
    "  \"costs\", sum('salary) over overRowCategory).select(\n",
    "  \"depName\", \"empNo\", \"name\", \"salary\", \"row_number\", \"costs\")\n",
    "running_total.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Range Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use range functions to change frame boundary.\n",
    "\n",
    "    Create with Window.partitionBy on one or more columns\n",
    "    It usually has orderBy so that the data in the frame is ordered.\n",
    "    Then followed by rangeBetween or rowsBetween\n",
    "    Each row will have a corresponding frame\n",
    "    Frame boundary can be controlled by rangeBetween or rowsBetween\n",
    "    Aggregate/Window functions can be applied on each row+frame to generate a single value\n",
    "\n",
    "There are two range window functions, here are the functions definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rowsBetween( start: Long, end: Long): WindowSpec\n",
    "\n",
    "def rangeBetween(start: Long, end: Long): WindowSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "both functions accept two parameters, [start, end] all inclusive. The parameters value can be Window.unboundedPreceding, Window.unboundedFollowing, and Window.currentRow. Or a value relative to Window.currentRow, either negtive or positive.\n",
    "\n",
    "rowsBetween get the frame boundary based on the row index in the window compared to currentRow. here are a few examples and it’s meaning.\n",
    "\n",
    "range-frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://knockdata.github.io/images/spark-window-function-range-frame.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+------+------------+------------+\n",
      "|depName  |empNo|name  |salary|salaries    |total_salary|\n",
      "+---------+-----+------+------+------------+------------+\n",
      "|develop  |7    |Astrid|4200  |[4200, 6000]|10200       |\n",
      "|develop  |8    |Saga  |6000  |[6000, 4500]|10500       |\n",
      "|develop  |9    |Freja |4500  |[4500, 5200]|9700        |\n",
      "|develop  |10   |Wilma |5200  |[5200, 5200]|10400       |\n",
      "|develop  |11   |Maja  |5200  |[5200]      |5200        |\n",
      "|sales    |1    |Alice |5000  |[5000, 4800]|9800        |\n",
      "|sales    |3    |Ella  |4800  |[4800, 4800]|9600        |\n",
      "|sales    |4    |Ebba  |4800  |[4800]      |4800        |\n",
      "|personnel|2    |Olivia|3900  |[3900, 3500]|7400        |\n",
      "|personnel|5    |Lilly |3500  |[3500]      |3500        |\n",
      "+---------+-----+------+------+------------+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "overCategory = org.apache.spark.sql.expressions.WindowSpec@441f6d21\n",
       "df = [depName: string, empNo: bigint ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[depName: string, empNo: bigint ... 5 more fields]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val overCategory = Window.partitionBy('depName).rowsBetween(Window.currentRow, 1)\n",
    "\n",
    "val df = empsalary\n",
    "        .withColumn(\"salaries\", collect_list('salary) over overCategory)\n",
    "        .withColumn(\"total_salary\", sum('salary) over overCategory)\n",
    "\n",
    "df.select(\"depName\", \"empNo\", \"name\", \"salary\", \"salaries\", \"total_salary\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+------+------------+------------+\n",
      "|depName  |empNo|name  |salary|salaries    |total_salary|\n",
      "+---------+-----+------+------+------------+------------+\n",
      "|develop  |8    |Saga  |6000  |[6000, 5200]|11200       |\n",
      "|develop  |10   |Wilma |5200  |[5200, 5200]|10400       |\n",
      "|develop  |11   |Maja  |5200  |[5200, 4500]|9700        |\n",
      "|develop  |9    |Freja |4500  |[4500, 4200]|8700        |\n",
      "|develop  |7    |Astrid|4200  |[4200]      |4200        |\n",
      "|sales    |1    |Alice |5000  |[5000, 4800]|9800        |\n",
      "|sales    |3    |Ella  |4800  |[4800, 4800]|9600        |\n",
      "|sales    |4    |Ebba  |4800  |[4800]      |4800        |\n",
      "|personnel|2    |Olivia|3900  |[3900, 3500]|7400        |\n",
      "|personnel|5    |Lilly |3500  |[3500]      |3500        |\n",
      "+---------+-----+------+------+------------+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "overCategory = org.apache.spark.sql.expressions.WindowSpec@648c123\n",
       "df = [depName: string, empNo: bigint ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[depName: string, empNo: bigint ... 5 more fields]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val overCategory = Window.partitionBy('depName).orderBy('salary desc).rowsBetween(Window.currentRow, 1)\n",
    "val df = empsalary\n",
    "            .withColumn(\"salaries\", collect_list('salary) over overCategory)\n",
    "            .withColumn(\"total_salary\", sum('salary) over overCategory)\n",
    "df.select(\"depName\", \"empNo\", \"name\", \"salary\", \"salaries\", \"total_salary\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean(avg) and median are commonly used in statistics. \n",
    "\n",
    "In certain cases median are more robust comparing to mean, since it will filter out outlier values.\n",
    "\n",
    "![title](https://knockdata.github.io/images/spark-window-function-mean-median.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+------+------------------------------+-------------+\n",
      "|depName  |empNo|name  |salary|salaries                      |median_salary|\n",
      "+---------+-----+------+------+------------------------------+-------------+\n",
      "|develop  |7    |Astrid|4200  |[4200, 4500, 5200, 5200, 6000]|5200         |\n",
      "|develop  |9    |Freja |4500  |[4200, 4500, 5200, 5200, 6000]|5200         |\n",
      "|develop  |10   |Wilma |5200  |[4200, 4500, 5200, 5200, 6000]|5200         |\n",
      "|develop  |11   |Maja  |5200  |[4200, 4500, 5200, 5200, 6000]|5200         |\n",
      "|develop  |8    |Saga  |6000  |[4200, 4500, 5200, 5200, 6000]|5200         |\n",
      "|sales    |3    |Ella  |4800  |[4800, 4800, 5000]            |4800         |\n",
      "|sales    |4    |Ebba  |4800  |[4800, 4800, 5000]            |4800         |\n",
      "|sales    |1    |Alice |5000  |[4800, 4800, 5000]            |4800         |\n",
      "|personnel|5    |Lilly |3500  |[3500, 3900]                  |3900         |\n",
      "|personnel|2    |Olivia|3900  |[3500, 3900]                  |3900         |\n",
      "+---------+-----+------+------+------------------------------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "overCategory = org.apache.spark.sql.expressions.WindowSpec@57dd055d\n",
       "df = [depName: string, empNo: bigint ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[depName: string, empNo: bigint ... 5 more fields]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val overCategory = Window.partitionBy('depName).orderBy('salary).rowsBetween(\n",
    "  Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "val df = empsalary.withColumn(\n",
    "  \"salaries\", collect_list('salary) over overCategory).withColumn(\n",
    "  \"median_salary\", element_at('salaries, (size('salaries)/2 + 1).cast(\"int\")))\n",
    "df.select(\"depName\", \"empNo\", \"name\", \"salary\", \"salaries\", \"median_salary\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "https://knockdata.github.io/spark-window-function/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
